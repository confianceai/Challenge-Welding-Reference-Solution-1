{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fc2a1a-843c-460f-84d3-8e05e04375eb",
   "metadata": {},
   "source": [
    "# Test your AI component compatibility\n",
    "\n",
    "This notebook demonstrates how your AI component can be built and tested to determine if it will be compatible with the pipeline evaluation process.\n",
    "\n",
    "This notebook provides an overview of how an AI component candidate will be evaluated. Even though it does not compute metrics, it gives a precise idea of how to build a compatible AI component and how it is used in inference for score generation.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The task is to build an AI component whose main objective is to predict the welding state from a given image.\n",
    "\n",
    "### Inputs to the AI component\n",
    "The AI component shall take the following inputs: \n",
    "- A list of numpy arrays representing the input images to process . \n",
    "- A list of dictionnaries containing a meta-description of the images.\n",
    "\n",
    "### Outputs of the AI component  \n",
    "It shall return a dictionnary with four keys {predictions , probabilities, OOD_score, explainabilities}\n",
    "    \n",
    "The first key is required:   \n",
    "- *predictions*:  The list of predicted welding state. The welding state can have three possible values: [OK, KO, UNKNOWN]\n",
    "    \n",
    "The following keys are not mandatory, but their presence will significantly contribute to the improvement of the quality score for the developed AI component:\n",
    "\n",
    "- *probabilities*:  The list of associated probabilities for each image. The list should have the format: [$P_{KO}$, $P_{OK}$, $P_{UNKNWON}$]  where $\\sum_{i \\in \\{\\text{OK, KO, UNKNOWN}\\}} P_i = 1$.\n",
    "\n",
    "- *OOD_scores*: The list of OOD scores predicted by the AI component for each images. This score (X) is a real positive number. If $0\\leq X < 1$ the image is considered as *In-Domain*, if $X >1$ the image is considered as *Out-of-Domain (OoD)*.\n",
    "\n",
    "- *explainabilities*: The list of explainabilities for each input image. An explainability is an intensity matrix (a matrix with values between 0 and 1) of the same size of the image tensor, which represents the importance of each pixel in the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3175750-1575-4d45-9b27-29e0f0377e20",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "Install the dependencies if it is not already done. For more information look at the [readme](../README.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf46c9b-99a7-4a67-aa2e-98aa3bfd1de1",
   "metadata": {},
   "source": [
    "##### For development on Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27e513d-181f-4153-9fb6-fdda54062850",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install a virtual environment\n",
    "# Option 1:  using conda (recommended)\n",
    "# !conda create -n venv python=3.12\n",
    "# !conda activate venv\n",
    "# !pip install torch==2.6.0\n",
    "\n",
    "# Option 2: using virtualenv\n",
    "# !pip install virtualenv\n",
    "# !virtualenv -p /usr/bin/python3.12 venv\n",
    "# !source venv_lips/bin/activate\n",
    "\n",
    "### Install the welding challenge package\n",
    "# Option 1: Get the last version from Pypi\n",
    "# !pip install 'challenge_welding'\n",
    "\n",
    "# Option 2: Get the last version from github repository\n",
    "# !git clone https://github.com/XX\n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41102f3e-02ec-4cd7-b132-1d0cd2a865dd",
   "metadata": {},
   "source": [
    "##### For Google Colab Users\n",
    "You could also use a GPU device from Runtime > Change runtime type and by selecting T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ac9858-d085-4aa0-b382-bd5bf3285300",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the welding challenge package\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "# !pip install 'challenge_welding'\n",
    "# !pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caee8b1e-98ed-404e-892e-66538b541f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Get the last version from github repository\n",
    "# !git clone https://github.com/XX\n",
    "# !pip install -U .\n",
    "# !pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f164041-0e31-453a-9d4b-5bb68de49d10",
   "metadata": {},
   "source": [
    "Attention: You may restart the session after this installation, in order that the changes be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139feb9f-9aca-4093-b760-988d4e6076cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import subprocess\n",
    "#repo_url = \"git+https://github.com/confianceai/Challenge-Welding-Starter-Kit.git\"\n",
    "#requirements_url = \"https://raw.githubusercontent.com/confianceai/Challenge-Welding-Starter-Kit/refs/heads/main/requirements.txt\"\n",
    "#subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", repo_url])\n",
    "#subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14d14d-7615-43b3-bd51-f6fec7935012",
   "metadata": {},
   "source": [
    "## Build your Ai component\n",
    "\n",
    "An AI component must be a buildable Python package . It should be a folder that contains at least the following files and folders:\n",
    "   ```\n",
    "    /\n",
    "    MANIFEST.in\n",
    "    setup.py\n",
    "    requirements.txt\n",
    "    challenge_solution/\n",
    "        AIcomponent.py\n",
    "        __init__.py\n",
    " ```\n",
    "\n",
    "You can refer to the [Requirements and evaluation process](../docs/Requirements_and_Evaluation_process.md) sections for more details.\n",
    "\n",
    "Only these files will be used by the evaluation pipeline to test your AI component. The names of files and folders must not be changed.\n",
    "The most important file is the AIcomponent.py file, which serves as the interface for your AI component. This interface will be the only part used by the evaluation pipeline to interact with your component. For this reason, this file must include specific methods and classes as required by the abstract class [AIComponent interface](../challenge_welding/AIComponent_interface.py) interface.\n",
    "\n",
    "You are free to add other files as needed to make your component work.\n",
    "\n",
    "The easiest way to build your AI component for the challenge is to start with the AI component template provided in the ```AIcomponent_template``` folder of this repository and complete the folder following the process described in its ```readme.md```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d0a02-ed55-43b8-83d0-6c7f001cef70",
   "metadata": {},
   "source": [
    "## Test an AI component compatibility\n",
    "\n",
    "In this section, we will test the AI component's compatibility with the evaluation pipeline of this challenge. The following lines will verify whether the proposed AI component can be loaded properly into the evaluation pipeline and used for inference computation on a given dataset. We do not provide the computation metrics function here, but all score metrics computation functions used by the evaluation pipeline are based solely on the inference results of the AI component across multiple evaluation datasets. Therefore, if inference works with your AI component, it will ensure that the score computation and the full evaluation process will work as well.\n",
    "\n",
    "In this example, we will use the reference solution provided within this challenge, which is accessible here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d83282-97e1-4e92-9fbc-ac3a22000cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set here the path of the AI component to test here, you can set a local filsystem path, or an url to a public git repository\n",
    "# You can replace it by the path to your own component to test\n",
    "\n",
    "AI_component_path= \"../Challenge-Welding-Reference-Solution-1\"  \n",
    "#AI_component_path=\"https://github.com/confianceai/Challenge-Welding-Reference-Solution-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef937f7-c985-4c30-bef2-f7e4dd1c143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, \"..\") # Uncomment this line For local tests without pkg installation, to make challenge_welding module visible \n",
    "from challenge_welding.user_interface import ChallengeUI\n",
    "from challenge_welding.inference_tools import TestAIComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53dbf4-03c4-40d3-9a69-c89d6b565817",
   "metadata": {},
   "source": [
    "## Launch the test pipeline\n",
    "\n",
    "The test pipeline take an AI component (the solution to test) and perform the following tasks\n",
    "\n",
    "- Install your AI component as a python package \n",
    "- Load the AI component of the solution you want to test\n",
    "- Apply inference on this AI component on one or many evaluation datasets. Each inference process on a dataset generate as output a dataframe( stored as a parquet file) containing evaluation dataset metadata extended with prediction results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e327bf-490d-4425-95c9-cba6940244b7",
   "metadata": {},
   "source": [
    "## Init the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb0681b-af28-40cc-bc93-87c539ec1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test pipeline\n",
    "myPipeline=TestAIComponent(proposed_solution_path=AI_component_path, # Set here the AI component path you want to evaluate\n",
    "                              meta_root_path=\"starter_kit_test_AI_comp_results\", # Set the directory here where pipeline results will be stored (inference results, and computed metrics)\n",
    "                              cache_strategy=\"local\", # \"local\" or \"remote\" .If set on \"local\", all image used for evaluation , will be locally stored in a cache directory. Else, image will be loaded directly from downloding\n",
    "                              cache_dir=\"test_cache\") # chosen directory for cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fddaef-6902-4025-b785-2e3ee76aa7a9",
   "metadata": {},
   "source": [
    "## Load your AI component into the evaluation environnement\n",
    "\n",
    "The load_proposed_solution() method below is divided into two main tasks:\n",
    "- Install the python package of your AI component --> ( execute the commande pip install AI_comp_path)\n",
    "- Call the load_model() method of your AIcomponent interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75924591-27d3-47b3-b6e1-7bf423321581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir C:\\SAUVEGARDES_FIN_CONFIANCE\\CSIA++\\Challenge\\Build2025\\Challenge-Welding-Reference-Solution-1\\challenge_solution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SAUVEGARDES_FIN_CONFIANCE\\CSIA++\\Challenge\\Build2025\\env-test-CODABENCH\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI component loaded\n"
     ]
    }
   ],
   "source": [
    "myPipeline.load_proposed_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd65bfb-9bf1-487b-a752-b82df711d9a6",
   "metadata": {},
   "source": [
    "## Load an evaluation dataset metadescription\n",
    "\n",
    "In the next cell, we load the metadata of the evaluation dataset we want to use to evaluate our AI component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a83a8a-c53d-42cf-8bda-feccbd9fba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://minio-storage.apps.confianceai-public.irtsysx.fr/challenge-welding/datasets/example_mini_dataset/metadata/ds_meta.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>class</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>welding-seams</th>\n",
       "      <th>labelling_type</th>\n",
       "      <th>resolution</th>\n",
       "      <th>path</th>\n",
       "      <th>sha256</th>\n",
       "      <th>storage_type</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>blur_level</th>\n",
       "      <th>blur_class</th>\n",
       "      <th>luminosity_level</th>\n",
       "      <th>external_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_92409</td>\n",
       "      <td>OK</td>\n",
       "      <td>22/01/20 12:49</td>\n",
       "      <td>c33</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'GN\\xd7\\xa7B\\x98\\xb0r\\xa4\\xdfn\\x8cT\\x8e:\\xc07...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>701.938341</td>\n",
       "      <td>blur</td>\n",
       "      <td>50.533365</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_67943</td>\n",
       "      <td>OK</td>\n",
       "      <td>20/02/20 23:53</td>\n",
       "      <td>c102</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b's\\xf6;3i-\\x10\\xfd8y\\xf2\\xe1\\xa6JQ\\x84`\\xc6\\x...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>715.670702</td>\n",
       "      <td>blur</td>\n",
       "      <td>47.050604</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_4843</td>\n",
       "      <td>OK</td>\n",
       "      <td>20/01/20 20:34</td>\n",
       "      <td>c20</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'\\xdbZ\\xb3\\x12e&amp;\\xd5\\x83\\x13*\\x87S\\xe1\\x19\\xc...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>715.857380</td>\n",
       "      <td>blur</td>\n",
       "      <td>46.204245</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_25309</td>\n",
       "      <td>OK</td>\n",
       "      <td>18/07/2022 20:18</td>\n",
       "      <td>c102</td>\n",
       "      <td>operator</td>\n",
       "      <td>[960, 540]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'/c\\xe3\\xd9\\xc8|&amp;\\xaf\\xb1}\\xf6\\xe3s\\xae\\xea\\x...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>869.513006</td>\n",
       "      <td>blur</td>\n",
       "      <td>34.359280</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_76144</td>\n",
       "      <td>OK</td>\n",
       "      <td>03/10/19 21:14</td>\n",
       "      <td>c20</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'\\xca%\\x0c\\x92\\x1f\\x0c\\x00\\xcc\\x02\\r\\xb8\\xf1\\...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>2676.246904</td>\n",
       "      <td>clean</td>\n",
       "      <td>46.256244</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id class         timestamp welding-seams labelling_type  \\\n",
       "0  data_92409    OK    22/01/20 12:49           c33         expert   \n",
       "1  data_67943    OK    20/02/20 23:53          c102         expert   \n",
       "2   data_4843    OK    20/01/20 20:34           c20         expert   \n",
       "3  data_25309    OK  18/07/2022 20:18          c102       operator   \n",
       "4  data_76144    OK    03/10/19 21:14           c20         expert   \n",
       "\n",
       "     resolution                                               path  \\\n",
       "0  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "1  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "2  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "3    [960, 540]  challenge-welding/datasets/example_mini_datase...   \n",
       "4  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "\n",
       "                                              sha256 storage_type data_origin  \\\n",
       "0  b'GN\\xd7\\xa7B\\x98\\xb0r\\xa4\\xdfn\\x8cT\\x8e:\\xc07...           s3        real   \n",
       "1  b's\\xf6;3i-\\x10\\xfd8y\\xf2\\xe1\\xa6JQ\\x84`\\xc6\\x...           s3        real   \n",
       "2  b'\\xdbZ\\xb3\\x12e&\\xd5\\x83\\x13*\\x87S\\xe1\\x19\\xc...           s3        real   \n",
       "3  b'/c\\xe3\\xd9\\xc8|&\\xaf\\xb1}\\xf6\\xe3s\\xae\\xea\\x...           s3        real   \n",
       "4  b'\\xca%\\x0c\\x92\\x1f\\x0c\\x00\\xcc\\x02\\r\\xb8\\xf1\\...           s3        real   \n",
       "\n",
       "    blur_level blur_class  luminosity_level  \\\n",
       "0   701.938341       blur         50.533365   \n",
       "1   715.670702       blur         47.050604   \n",
       "2   715.857380       blur         46.204245   \n",
       "3   869.513006       blur         34.359280   \n",
       "4  2676.246904      clean         46.256244   \n",
       "\n",
       "                                       external_path  \n",
       "0  http://minio-storage.apps.confianceai-public.i...  \n",
       "1  http://minio-storage.apps.confianceai-public.i...  \n",
       "2  http://minio-storage.apps.confianceai-public.i...  \n",
       "3  http://minio-storage.apps.confianceai-public.i...  \n",
       "4  http://minio-storage.apps.confianceai-public.i...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this example we will choose a small dataset\n",
    "\n",
    "ds_name=\"example_mini_dataset\"\n",
    "\n",
    "# Load all metadata of your dataset as a pandas dataframe, (you can point to a local cache metafile instead of original one pointing on remote repository)\n",
    "\n",
    "my_challenge_UI=ChallengeUI()\n",
    "evaluation_ds_meta_df=my_challenge_UI.get_ds_metadata_dataframe(ds_name)\n",
    "\n",
    "display(evaluation_ds_meta_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e70108-edb7-4de7-85ab-8048a737aaa0",
   "metadata": {},
   "source": [
    "##  Perform inference on an evaluation dataset\n",
    "\n",
    "We pass the evaluation_dataframe in the method below. It use the loaded AI component to perform inference of each sample referenced in the evaluation dataframe and add the inference results as new columns\n",
    "\n",
    "The predict method of your AI component will be called with the **device** parameter on \"cuda\" . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df7d85-68dc-4f3f-8456-91758c06f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of  batch to process for inference :  20  , start processing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▉                                                                                                                 | 1/20 [00:02<00:50,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▉                                                                                                           | 2/20 [00:04<00:41,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▊                                                                                                     | 3/20 [00:06<00:35,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▊                                                                                               | 4/20 [00:08<00:32,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████▊                                                                                         | 5/20 [00:10<00:29,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████▋                                                                                   | 6/20 [00:12<00:27,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████▋                                                                             | 7/20 [00:14<00:25,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████▌                                                                       | 8/20 [00:16<00:23,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████████▌                                                                 | 9/20 [00:18<00:21,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████                                                           | 10/20 [00:19<00:19,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████▉                                                     | 11/20 [00:21<00:17,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 12/20 [00:23<00:15,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████████████████████████████▋                                         | 13/20 [00:25<00:13,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "result_df=myPipeline.perform_grouped_inference(evaluation_dataset=evaluation_ds_meta_df, # dataframe containing metadescription of your evaluation ds\n",
    "                                               results_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\", # path to file that will contains inference_results\n",
    "                                               batch_size=150 # You can group inference by batch if you want\n",
    "                                              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12ffb1-1182-4e6d-b23c-578231d12199",
   "metadata": {},
   "source": [
    "You can see the inference results below. See that new column has been added :\n",
    "- \"predicted_state\" imported from \"predicitions\" key of your AI component predict method output dictionnary\n",
    "- \"scores KO\" imported from \"probabilities\" key of your AI component predict method output dictionnary\n",
    "- \"scores OK\" imported from \"probabilities\" key of your AI component predict method output dictionnary\n",
    "- \"score OOD\" imported from \"OOD scores\" key of your AI component predict method output dictionnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18ec49-b6ce-413e-b209-b75634a44193",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78948c8f-361c-475f-ad3a-729022f181e6",
   "metadata": {},
   "source": [
    "Check the output dataframe contain columns corresponding to ouptut fields filled with correct values. \n",
    "Here the reference component tested shall create columns named, \n",
    "- predicted_states\n",
    "- scores_KO\n",
    "-  scores OK\n",
    "-   OOD_scores\n",
    "-   \n",
    "If your result dataframe is correct and the parquet is well created, then your AI comp is compatible with the evaluation pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfe596-6ef5-4357-8d36-fe8a30392bf6",
   "metadata": {},
   "source": [
    "# Evaluation process\n",
    "\n",
    "The Trustworthy AI Challenge aim to build a trustworthy AI-Component that assists in weld seam conformity qualification. The evaluation of a trustworthy AI-Component will be done across several dimensions of trustworthy to ensure it reliability, robustness, and efficiency facing real observation that may be affected by hazards. The evaluation framework consists of six \"trust-attributes\" : \\textbf{Performance, uncertainty, robustness, ood monitoring, generalization, and drift}.  These aspects are some of the trust attributes that may determine the AI system’s ability to operate effectively in real-world scenarios as for example be robust to small environmental hazard, generalize across datasets, express confidence or be able to face anomalies.\n",
    "\n",
    "From the predictions made by the developed AI component on many evaluation datasets, we will compute a set of different evaluation criteria as discussed below:\n",
    "\n",
    "- **Performance metrics**: Measure the gain brought the AI component compared to a human only qualification process. This metrics is based on the confusion matrix and penalize strongly false negative predictions. \n",
    "\n",
    "- **Uncertainty metrics**: Measure the ability of the AI component to produce a calibrated prediction confidence indicator expressing risk of model errors.\n",
    "\n",
    "- **Robustness metrics**: Measure the ability of the AI component to have invariant output is images have slight perturbations (blut, luminosity, rotation, translation)\n",
    "\n",
    "- **OOD-Monitoring metrics**: Measure the ability of the AI component to detect if an input image is ood, and gives the appropriate output ->Unknown\n",
    "\n",
    "- **Generalisation metrics**: Measure the ability of the AI component to generalize to a unseen context.\n",
    "\n",
    "- **Drift metrics**: Measure the ability of the AI component to generalize to a unseen context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166083a-ff58-4434-b509-de6fa7bade61",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f2bed-8c73-4e71-bf29-69e740a6bc8f",
   "metadata": {},
   "source": [
    "# Trustworthy ML Challenge Evaluation Protocol\n",
    "\n",
    "## Multi-Criteria Aggregation Methodology\n",
    "\n",
    "The ML-Trustworthy evaluation of the submitted AI component follows a **multi-criteria aggregation methodology** designed to ensure a fair and reliable assessment of various **trust attributes**.  \n",
    "The table below illustrates the principle of metrics aggregation:\n",
    "\n",
    "![image](./docs/assets/Metric_tabular.png)\n",
    "\n",
    "## Example of a comparative results table for four fictional submissions.\n",
    "\n",
    "The following table illustrates a performance overview of four different virtual solutions which were actually evaluated (using manually constructed inference files) using the trustworthy AI pipeline. The indicator color codes are for illustrative purposes only.\n",
    "\n",
    "Among these four submissions:\n",
    " - **Solu-Perfect**: The ideal solution, achieving perfect scores in both performance and all trust-related attributes.\n",
    "- **Solu-No-Trust**: A realistic solution without any dedicated mechanisms to address trustworthy AI concerns.\n",
    "- **Solu-With-Trust**: The same base solution as Solu-No-Trust, but enhanced with mechanisms for handling uncertainty, robustness, OOD monitoring, and drift management.\n",
    "- **Solu-Random**: A baseline solution that returns random predictions.\n",
    "\n",
    "We observe that the **Solu-Perfect** solution achieves a perfect score across all metrics. Both **Solu-No-Trust** and **Solu-With-Trust** show identical scores in terms of Performance and Generalization. However, Solu-With-Trust significantly improves its trustworthiness scores across other attributes such as Uncertainty, Robustness, Monitoring, and Drift Management.\n",
    "\n",
    "![image](./docs/assets/Tabular_metrique.png)\n",
    "\n",
    "## ML-Trustworthy Evaluation design\n",
    "\n",
    "The evaluation protocol was designed to assess both **performance** and **trustworthiness requirements**, based on the **Operational Design Domain (ODD)** derived from operational needs linked to the AI component's automated function (i.e., assistance in weld validation).\n",
    "\n",
    "After identifying the relevant **trust attributes** (e.g., *robustness*) associated with specific **trust properties** (e.g., *output invariance under blur perturbation*), the evaluation methodology was structured into the following stages:\n",
    "\n",
    "- **Evaluation Specification**  \n",
    "  What specific model behaviors do we want to assess and validate?\n",
    "\n",
    "- **Evaluation Set Specification**  \n",
    "  What kind of data must be used or constructed to test whether the model exhibits the expected behavior under specific conditions?\n",
    "\n",
    "- **Evaluation Set Design**  \n",
    "  What data should be selected or generated to build these evaluation sets?\n",
    "\n",
    "- **Evaluation Set Validation**  \n",
    "  How can we ensure that the evaluation datasets are reliable and representative of the scenarios being analyzed?\n",
    "\n",
    "- **Criteria Specification**  \n",
    "  What criteria should be defined to measure the presence or absence of the expected behavior?\n",
    "\n",
    "- **Metrics Design**  \n",
    "  What metrics can be used to quantify these criteria?\n",
    "\n",
    "- **Trust-KPI Design**  \n",
    "  How can these criteria be aggregated into a **Trust-KPI** for each trust attribute?\n",
    "\n",
    "## Steps of the Metrics and Trust-KPI Computation\n",
    "\n",
    "The aggregation process consists in several key steps:\n",
    "\n",
    "In this section, $\\alpha_{i}$ $\\beta_{i}$ and $k_{i}$ are weigthing or scaling coeficients used for the multicriterions aggregation.\n",
    "\n",
    "\n",
    "### 1. Computation of Metrics Related to Trust Attributes\n",
    "\n",
    "- Several metrics are computed for each attribute using specific evaluation datasets, in order to capture different aspects of the attribute’s performance.\n",
    "- These evaluation datasets are either selected or synthetically generated to test distinct behavioral criteria.\n",
    "\n",
    "### 2. Normalization of Attribute Metrics\n",
    "\n",
    "- All attribute-specific metrics are normalized to a score within the range \\[0, 1\\], where **1** represents the best possible performance.\n",
    "- Normalization is performed using appropriate transformations (e.g., sigmoid functions, exponential decay), depending on the nature of each metric.\n",
    "\n",
    "### 3. Trust-KPI Aggregation\n",
    "\n",
    "- For each attribute denoted X, a specific aggregation function combines the k-th normalized X metrics into a single **trust-KPI** denoted $I_X$.\n",
    "- This allows for a comprehensive representation of the model’s performance with respect to each trust attribute.\n",
    "\n",
    "$$ I_X = agg(X_{metric_1},..,X_{metric_k})$$\n",
    "\n",
    "For example, if X is the attribute \"performance\":  $X_{metric_1}=OP$, $X_{metric_2}=ML$, and $X_{metric_3}=Time$\n",
    "\n",
    "### 4.Piecewise Linear Rescaling of Trust-KPIs\n",
    "\n",
    "  - To ensure consistency and comparability across attributes, each KPI undergoes a **piecewise linear rescaling**.\n",
    "  - This rescaling takes into account both predefined performance and confidence requirements.\n",
    "  - This rescaling accounts for predefined performance and confidence thresholds, aligning the raw scores with evaluation constraints.\n",
    "\n",
    "$$f'(x) =\n",
    "\\begin{cases}\n",
    "\\frac{\\beta_1}{\\alpha_1} f(x), & 0 \\leq f(x) < \\alpha_1 \\\\\n",
    "\\frac{\\beta_2 - \\beta_1}{\\alpha_2 - \\alpha_1} (f(x) - \\alpha_1) + \\beta_1, & \\alpha_1 \\leq f(x) \\leq \\alpha_2 \\\\[8pt]\n",
    "\\frac{1 - \\beta_2}{1 - \\alpha_2} (f(x) - \\alpha_2) + \\beta_2, & \\alpha_2 < f(x) \\leq 1\n",
    "\\end{cases}$$\n",
    "\n",
    "### 5. Weighted Aggregation of Trust-KPIs\n",
    "  - The rescaled attribute KPIs are then aggregated into a **final evaluation score** using a **weighted mean**.\n",
    "  - Each weight reflects the relative importance of its corresponding attribute within the overall trustworthy AI assessment.\n",
    "$$ score= \\alpha_1*I_{perf} + \\alpha_2*I_{U} + \\alpha_3*I_{rob} + \\alpha_4*I_{ood} + \\alpha_5*I_{gen}+\\alpha_6*I_{drift}$$\n",
    "\n",
    "### 6. Purpose of the Aggregation Protocol\n",
    "\n",
    "The goal of this aggregation process is to produce a single, comprehensive trust score that captures the system’s performance across six key trust attributes. Each of these attributes is assessed through multiple criteria, measured with relevant metrics and normalized to reflect their practical impact.\n",
    "\n",
    "## Trust-KPI and metrics by attribute.\n",
    "\n",
    "### Performance attribute\n",
    "\n",
    "**Purpose**: Measures the model's predictive accuracy and efficiency, ensuring it meets baseline expectations in a controlled environment.\n",
    "\n",
    "**Evaluation sets**: Standard ML evaluation set based on a representative 20% split of the dataset.\n",
    "\n",
    "**Metrics**:\n",
    "  - **OP-Perf** (Operational Performance): Evaluates model performance through an operational view using confusion-matrix-based metrics that account for the cost of different error types and weld criticality.\n",
    "\n",
    "       $$OP = \\sum_{k}^{|N|} \\sum_{i}^{true_{class}} \\sum_{j}^{pred_{class}} \\mathbb{1}_{Top_{class}(\\hat{y}_k)=j} * cost(i,j,k,k_{seam}) $$\n",
    "\n",
    "  where N is the number of sample in the evaluation datasset and $k_{seams}$ is the name of the welding-seam\n",
    "\n",
    "\n",
    "  - **ML-Perf** (Machine Learning Performance): Assesses performance using standard ML metrics such as precision.\n",
    "    $$ ML = \\frac{\\sum_{i=1}^{N} \\mathbb{1} (y_i = 1 \\land \\hat{y}_i = 1)}{\\sum_{i=1}^{N} \\mathbb{1} (\\hat{y}_i = 1)}$$\n",
    "\n",
    "where $y_i$ is the ground truth and $\\hat{y}_i$ is the AI component prediction\n",
    "\n",
    "\n",
    "  - **Inference Time (Times)**: Measures computational efficiency and runtime.\n",
    "\n",
    "**Performance-KPI**: Combines OP-Perf and ML-Perf using a weighted average, penalized by inference time to reflect operational constraints.\n",
    "$$ I_{perf}=\\frac{(\\alpha_{op} e^{-k_c OP} + \\alpha_{ml} ML)}{1 + k_t ln(1+t)} $$\n",
    "\n",
    "where $t$ is the inference time\n",
    "\n",
    "### Uncertainty assessement\n",
    "**Purpose** : Evaluates the AI component’s ability to express meaningful and calibrated uncertainty, helping assess the risk of decision errors.\n",
    "\n",
    "**Evaluation sets**: Standard ML evaluation set based on a representative 20% split of the dataset.\n",
    "\n",
    "**Metrics**:\n",
    "  - **U-OP** (Uncertainty Operational Gain): Relative measures of the virtual gain (in operational term) to consider probabilistic outputs compared to hard outputs predictions in relation to the gap between the perfect solution and the current hard outputs predictions.\n",
    "  $$c^{U} = \\sum_{k}^{|N|} \\sum_{i}^{true_{class}} \\sum_{j}^{pred_{class}} \\hat{y}_k(j) * cost(i,j,k,k_{seam}) $$\n",
    "\n",
    "  $$ UOP = \\frac{(c^{U} - c^{op})}{(c^{op} - c^{op}_{perfect})}$$\n",
    "\n",
    "  - **U-Calib** (Calibration Quality): Evaluates how well predicted probabilities align with actual error rates (e.g., Expected Calibration Error).\n",
    "    $$ UCalib = \\sum_{m=1}^{M} \\frac{|B_m|}{N} acc(B_m) - conf(B_m)$$\n",
    "\n",
    "Fore more information, see Expected calibration error definition :  lien wiki\n",
    "\n",
    "**Uncertainty-KPI** : Combines Uncertainty Operational Gain with calibration error.\n",
    "$$I_{U} = e^{k_{UOP}} * (1 - UCalib)^{k_{UCalib}} $$\n",
    "\n",
    "### Robustness\n",
    "**Purpose**: Assesses model stability under perturbations such as blur, lighting variation, rotation, and translation.\n",
    "\n",
    "**Evaluation sets**: Generated by applying synthetic perturbations to a weld-balanced subset of the standard evaluation set.\n",
    "\n",
    "![image](./images/Blur_illu.png)\n",
    "\n",
    "**Metrics**:\n",
    "   - **Blur Robustness** : Aggregation (AUC) of the ML-performance (Precision score) across increasing perturbation levels .\n",
    "   - **Luminance Robustness** : Aggregation (AUC) of the ML-performance (Precision score) across increasing perturbation levels.\n",
    "   - **Rotation Robustness** : Aggregation (AUC) of the ML-performance (Precision score) across increasing perturbation levels.\n",
    "   - **Translation Robustness**: Aggregation (AUC) of the ML-performance (Precision score) across increasing perturbation levels.\n",
    "\n",
    "$$ r^x = Auc(ML_{\\delta_1}/,..., ML_{\\delta_k}) $$ \n",
    "\n",
    "where $x\\in \\{blur,lum,rot,trans\\}$ and $\\delta_k$ are the different perturbation levels\n",
    "\n",
    "**Robustness-KPI** : Weighted aggregation of robustness scores across all perturbation types.\n",
    "\n",
    "$$ I_{rob} = \\sum_{i \\in {blur,lum,rot,trans}} \\alpha_{r_i} * r^i $$ \n",
    "\n",
    "### OOD-Monitoring \n",
    "\n",
    "**Purpose**: Evaluates the model's ability to detect and handle out-of-distribution (OOD) inputs.\n",
    "\n",
    "**Evaluation sets**: Includes both synthetic and real OOD datasets with a balanced mix of normal and OOD samples. Real OOD samples are manually selected, and synthetic OOD samples are generated through transformations.\n",
    "\n",
    "![image](./docs/assets/Ood_illu.png)\n",
    "\n",
    "**Metrics**\n",
    "  - **Real-OOD score** : AUROC on the real OOD evaluation set denoted $OOD_{real}$.\n",
    "  - **Syn-OOD score** :AUROC on the synthetic OOD evaluation set $OOD_{syn}$.\n",
    "  \n",
    "\n",
    "**OOD-Monitoring KPI**: Weighted average of real and synthetic OOD detection performance.\n",
    "$$I_{ood} = \\alpha_{syn}*OOD_{syn} + \\alpha_{real}*OOD_{real}$$\n",
    "\n",
    "### Generalization \n",
    "**Purpose**: Measures the model’s ability to generalize to unseen weld types that share characteristics with the training set.\n",
    "\n",
    "**Evaluation sets**: Built using data from weld types excluded during training but with similar visual/structural traits.\n",
    "\n",
    "![image](./docs/assets/Gen_illu.png)\n",
    "\n",
    "**Metrics**:\n",
    "  - **OP-Perf-g** Operational performance on the generalization set.\n",
    "  - **ML-Perf-g** ML performance (e.g., precision) on the generalization set.\n",
    "\n",
    "**Generalization-KPI**: Aggregated from OP-Perf-g and ML-Perf-g.\n",
    "$$I_{gen} = \\alpha_{op}*e^{-k_{op}*OP_g} + \\alpha_{ml}*ML_{g}$$\n",
    "\n",
    "Subindice $g$ in $ML_g$ or $OP_g$ means that metrics are computed on the generalization dataset.\n",
    "\n",
    "### Data-Drift handling\n",
    "**Purpose**: Evaluates both the robustness and OOD detection of the model in response to gradual data drift.\n",
    "\n",
    "**Evaluation sets**: Constructed by applying increasing levels of synthetic perturbations to a normal data sequence, simulating drift. Final segments are manually labeled as OOD.\n",
    "\n",
    "![image](./docs/assets/Drift_illu.png)\n",
    "\n",
    "**Metrics**:\n",
    "  - Perf-OP-d : Operational performance under drift.\n",
    "  - OOD-d: \"OOD-Detection score\" : AUROC on the drift-induced OOD subset.  \n",
    "\n",
    "**Data-Drift-KPI**: Combines performance and detection ability during simulated drift.\n",
    "$$I_{drift} = \\alpha_{OP_{d}} * e^{-k_{op} * OP_{d}} + \\alpha_{OOD_{d}}*OOD_{d}$$\n",
    "\n",
    "\n",
    "where subindice $d$ means that the metrics are computed only on the drifted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38026ba8-145e-46f9-9962-3a6335f7797b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca335e-803e-4a9e-91a8-a32c3e5ee80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624008b-7367-467a-b1fe-eb83443123de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dca966-930e-45eb-9507-5b7fe0557d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-codabench",
   "language": "python",
   "name": "env-codabench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
